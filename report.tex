\documentclass{article}

\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[english, lithuanian]{babel}
\usepackage{float}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{tabularray}
\usepackage{datetime}
\usepackage{comment}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{derivative}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{color}
\usepackage{rotating}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{pythonhighlight}

\DeclareUnicodeCharacter{2212}{-}
\selectlanguage{lithuanian}

\begin{document}
\newlength{\mywidth}
\settowidth{\mywidth}{Darbo vadovas:}
\begin{titlepage}
    \vskip 20pt
    \centerline{\bf \large VILNIAUS UNIVERSITETAS}
    \bigskip
    \centerline{\large \textbf{MATEMATIKOS IR INFORMATIKOS FAKULTETAS}}
    \vskip 120pt
    \centerline{\bf \Large \textbf{Laboratorinis darbas 2}}
    \vskip 50pt
    \begin{center}
        {\bf \LARGE Optimizavimas be apribojimų}
    \end{center}
    \bigskip
    \bigskip
    \centerline{\Large Nikita Gainulin}
    \vskip 90pt
    \vskip 200pt
    \centerline{\large \textbf{VILNIUS 2024}}
\end{titlepage}

\tableofcontents

\clearpage
\section{Įvadas}
Savo ankstesniame laboratoriniame darbe gilinausi į vienmatį optimizavimą. Kaip nustačiau, dauguma šių algoritmų remiasi tuo, kad naudotojas turi pasirinkti konkretų intervalą, kuriame bus ieškoma minimumo taško. Tačiau šį kartą nagrinėsiu optimizavimą be apribojimų, kurio algoritmams, kaip galima spėti iš pavadinimo, nebūtinai reikia iš anksto nustatyto intervalo, nes jų leistinoji sritis sutampa su visa n-mate Euklido erdve $\mathbb{R}^n$ ir jiems veikti reikės kitokio kintamųjų rinkinio. Be to, šiame laboratoriniame darbe taip pat bandysiu optimizuoti konkrečią problemą, panašią į tas, kurios iš tikrųjų pasitaiko realiame pasaulyje.
\section{Nagrinėjama problema}
Prieš tęsdami nustatysime tikrąją problemą, kurią bandysiu optimizuoti. Štai kaip ji skamba:

\textbf{Kokia turėtų būti stačiakampio gretasienio formos dėžė, kad vienetiniam paviršiaus plotui jos tūris būtų maksimalus?}

Pirmiausia, atsižvelgiant į šio laboratorinio darbo temą, tikslo funkciją būtina aprašyti taip, kad pats optimizavimo uždavinys būtų sudarytas be apribojimų, t. y. $\min f(X)$. Kaip jau žinome, standartinė tūrio formulė yra tokia:
\begin{equation*}
    V = a\cdot b\cdot c,
\end{equation*}
kur $a, b$ ir $c$ yra mūsų stačiakampio gretasienio ilgis, plotis ir aukštis. Tačiau dėl paprastumo dirbsime su tūrio kvadratu, nes vėliau atlikdami pakeitimą gausime daugianarę išraišką, todėl bus daug lengviau rasti išvestines ir kritinius taškus. Apibrėžkime:
\begin{itemize}
    \item $x_{1} = 2ab$, priekinės ir galinės sienų plotų suma;
    \item $x_{2} = 2bc$, šoninių sienų plotų suma;
    \item $x_{3} = 2ac$, viršutinės ir apatinės sienų plotų suma;
\end{itemize}

Iš čia:
\begin{itemize}
    \item $ab = \frac{x_{1}}{2}$;
    \item $bc = \frac{x_{2}}{2}$;
    \item $ac = \frac{x_{3}}{2}$;
\end{itemize}

Kadangi mūsų užduotis yra maksimaliai padidinti dėžės tūrį, tenkantį vienam paviršiaus ploto vienetui, mūsų reikalavimas tampa $x_{1} + x_{2} + x_{3} = 1$. Čia galime išreikšti $x_{3} = 1 - x_{1} - x_{2}$.Taip pradinį optimizavimo uždavinį transformuojame į neapribotą optimizavimo uždavinį. Kadangi mus domina tik tūrio kvadratas, toliau atliekami tokie veiksmai:
\begin{equation}\label{eq:1}
    V^2 = (abc)^2 = aabbcc = \frac{x_{1}}{2}\cdot \frac{x_{2}}{2}\cdot \frac{x_{3}}{2} = \frac{1}{8}\cdot x_{1}x_{2}x_{3} = \frac{1}{8}\cdot x_{1}x_{2}\cdot (1-x_{1}-x_{2}) = \frac{1}{8}\cdot (x_{1}x_{2}-x_{1}^2x_{2}-x_{1}x_{2}^2)
\end{equation}

Kadangi mūsų tikslas yra optimizuoti uždavinį ieškant minimumo taško, kuriame dėžės tūris yra didžiausias, turime padauginti \ref{eq:1} formulę iš -1, nes didžiausia V² vertė atitinka mažiausią -V². Štai tai ir gauname mūsų objektinę funkciją:
\begin{equation}\label{eq:2}
    f(x) = -\frac{1}{8}(x_{1}x_{2}-x_{1}^2x_{2}-x_{1}x_{2}^2)
\end{equation}
\subsection{Objektinė funkcija ir jos gradientas}
Ankstesniame skyriuje nustatėme tokią optimizavimo uždavinio tikslo funkciją (\ref{eq:2}):
\begin{equation*}
    f(x) = -\frac{1}{8}(x_{1}x_{2}-x_{1}^2x_{2}-x_{1}x_{2}^2)
\end{equation*}

Dviem iš trijų algoritmų, kuriuos nagrinėsiu šiame laboratoriniame darbe, reikalingas vadinamasis tikslo funkcijos gradientas. \textbf{Gradientas} - vektorius, sudarytas iš funkcijos dalinių išvestinių,
apskaičiuotų taške $x$.
\begin{equation*}
    \nabla f(x) = (\pdv{f(x)}{x_{1}},\dots,\pdv{f(x)}{x_{n}})
\end{equation*}

Mūsų tikslinei funkcijai tai gana paprasta - ją tereikia diferencijuoti pagal $x_{1}$ ir $x_{2}$ atskirai:
\begin{equation*}
    \pdv{f(x)}{x_{1}} = -\frac{1}{8}(x_{2}-2x_{1}x_{2}-x_{2}^2)
\end{equation*}
\begin{equation*}
    \pdv{f(x)}{x_{2}} = -\frac{1}{8}(x_{1}-2x_{1}x_{2}-x_{1}^2)
\end{equation*}

Taigi, gavome savo tikslo funkcijos gradientą:
\begin{equation}\label{eq:3}
    \nabla f(x) = (-\frac{1}{8}(x_{2}-2x_{1}x_{2}-x_{2}^2), -\frac{1}{8}(x_{1}-2x_{1}x_{2}-x_{1}^2))
\end{equation}

Savo patogumui sukūriau tikslo funkcijos ir jos gradiento Python klasę, nes ji leidžia man įdiegti vidinį skaitiklį, kuris leidžia daug tiksliau apskaičiuoti visą funkcijos iškvietimą, taip pat funkciją, kuri jį atstato. Taip man nebereikės gaišti laiko ieškant vietos kode, kur rankiniu būdu padidinti skaitiklį - tai buvo vienas iš mano pirmojo laboratorinio darbo aplaidumų.
\inputpythonfile{objfunc.py}
\section{Optimizavimo be apribojimų metodai ir jų algoritmai}
Šiame skyriuje papasakosiu apie pačius metodus, kaip veikia jų algoritmai, ir pateiksiu savo asmeninę kiekvieno algoritmo interpretaciją.
\subsection{Gradientinio nusileidimo metodas}
Paprastai nusileidimo metodai pagrįsti informacija apie tikslo funkcijos $f(x)$ pirmąją ir antrąją dalines išvestines. Pats gradientas nukreiptas funkcijos greičiausio augimo kryptimi, todėl, norėdami rasti minimumo tašką abiem nusileidimo metodais, žengsime būtent antigradiento kryptimi. Tai ir yra esminė priežastis kodėl mūsų objektinės funkcijos (\ref{eq:2}) priekyje yra minuso ženklas.

Panagrinėkime gradientinio nusileidimo metodą. Toliau pateikta formulė laikoma jo bei sekančio nusileidimo metodo esme:
\begin{equation}\label{eq:4}
    x_{i+1} = x_{i} - \gamma\nabla f(x_{i}),
\end{equation}
kur $x_{i}$ ir $x_{i+1}$ - atitinkamai vienos ir sekančios iteracijų bandomieji taškai, $\gamma$ - žingsnio daugiklis bei $\nabla f(x_{i})$ - objektinės funkcijos gradientas taške $x_{i}$. Pradėję nuo nurodyto pradinio taško $x_{0}$, taikydami pirmiau pateiktą formulę, mūsų $x_{i}$ iteracija po iteracijos artėja prie minimumo taško. Svarbu pažymėti, kad nors mūsų žingsnio daugiklis $\gamma$ tiesiogiai niekada nesikeičia, artėjimo greitis $(-\gamma\nabla f(x_{i}))$ nėra pastovus ir kiekvieną iteraciją tampa vis mažesnis dėl to, kad mažėja gradiento norma.

Tyrinėdamas ir bandydamas parašyti Python gradiento nusileidimo algoritmo realizaciją, pastebėjau įdomų dalyką - dauguma tyrėjų mėgsta naudoti šiuos šešis žingsnio daugiklius $\gamma$: 0.001, 0.003, 0.01, 0.03, 0.1 ir 0.3. Pirmieji du man pasirodė gana maži, todėl konkrečiai šiam metodui nusprendžiau nustatyti ne 200, o 1000 iteracijų ribą, nes teoriškai net ir tokio kiekio gali nepakakti, kad su tam tikrais koeficientais būtų pasiektas minimumo taškas. Taigi, štai kaip atrodo mano gradientinio nusileidimo algoritmas:
\inputpythonfile{gradDescent.py}
\subsection{Greičiausio nusileidimo metodas}
Apskritai stačiausio nusileidimo metodas yra labai panašus į gradiento nusileidimo - taip pat artėja prie minimumo taško iteruojant per formulę (\ref{eq:4}), tačiau šį kartą pats algoritmas, naudodamas išorinį, nustato optimaliausią žingsnio daliklį per iteraciją. Savo atveju nusprendžiau naudoti auksinio pjūvio algoritmą iš pirmojo laboratorinio darbo, nes man jį daug lengviau įgyvendinti. Tačiau galima teigti, kad taip elgdamasis atmetu neribotąją nusileidimo algoritmų savybę dėl to, kad auksinio pjūvio algoritmas reikalauja intervalo, tačiau tai netiesa, nes paieška tik padeda rasti efektyviausią žingsnio dydį nusileidimo kryptimi ir neriboja, kur greičiausio nusileidimo algoritmas gali eiti bendroje objektinės funkcijos erdvėje.

Taigi, aukso pjūvio intervalai, kuriuos naudosiu, yra standartinis [0,1] ir tie, kuriuos radau bandymų būdu - [0,7] ir [0,20]. Intervalų pasirinkimą plačiau aptarsiu, kai pereisime prie algoritmo rezultatų. Štai Python įgyvendinimas:
\inputpythonfile{steepDescent.py}
\subsection{Deformuojamo simplekso (Nelder-Mead) metodas}
Skirtingai nei ankstesni du metodai, Nelderio-Medo, arba kaip jis paprastai vadinamas deformuojamo simplekso, nesiremia tikslo funkcijos gradientu, o naudoja figūras, sudarytas iš taškų, kurie iteracijų metu palaipsniui artėja prie minimumo taško. Figūra priklauso nuo paieškos erdvės matmens, kuris nustatomas pagal tai, kiek kintamųjų bandome optimizuoti, tai yra $n$ - įvesto $x_{0}$ ilgio, ir griežtai vadovaujasi $n+1$ formule. Mūsų atveju tai bus trikampis. Taigi algoritmui nustačius pradinį trikampį, jo taškai išrikiuojami nuo geriausio, tai yra arčiausiai minimumo taško pagal tikslo funkcijos vertę, iki blogiausio. Tada sukuriamas dar vienas taškas, vadinamas centroidu - visų taškų, išskyrus blogiausią, centro taškas. Po to algoritmas atspindi blogiausią tašką per centrą, tarsi jį apverčia, ir tada jį išsaugo arba, jei tas naujas taškas yra arčiau minimumo, dar labiau išplečia. Tačiau jei atspindėtas taškas yra toliau nei antras blogiausias - jis perkeliamas arčiau centroido. Ir šie veiksmai su figūromis tęsiasi kiekvieną iteraciją, kol taškai pakankamai priartėja vienas prie kito ir atitinkamai prie minimumo taško arba algoritmas išnaudoja leistiną iteracijų skaičių.

Taip atrodo mano deformuojamo simplekso įgyvendinimas Python kalba:
\inputpythonfile{simplex.py}
\section{Rezultatai ir jų analyzė}
Prieš pradedant rezultatų analizę, svarbu nustatyti kelias pagrindines sąvokas, kurias naudojau ir ankstesniame laboratoriniame darbe, taip pat pradinius taškus, kuriuos nagrinėsiu.

Optimizavimo metodo algoritmo \textbf{greitis} - tai jo vidinių iteracijų kiekis, per kurį randamas tikslus minimumo taškas (arba intervalas, kuriame yra tas taškas ir kuris yra mažesnis už iš anksto nustatytą $\varepsilon$). Paprastai kuo mažiau iteracijų algoritmui reikia minėtam mažiausiam taškui pasiekti, tuo jis yra greitesnis. 

Optimizavimo metodo algoritmo \textbf{efektyvumas} - tikslo funkcijos bei jos gradiento iškvietimų kiekis. Kuo mažiau kartų iškviečiame funkciją arba jos gradientą tam tikro taško reikšmei apskaičiuoti, tuo mažiau išteklių sunaudojame algoritmui vykdyti, vadinasi, tuo efektyviau naudoti tam tikrą metodą. 

Trys pradiniai taškai $x_{0}$, kuriuos įvesiu į algoritmus, yra šie: [0,0], [1,1] ir [0.5,0.7].

Dabar, kai pagrindinės sąvokos ir nuliniai taškai apibrėžti, pereikime prie analizės.
\subsection{Gradientinio nusileidimo metodo rezultatai ir vizualizacija}
Taikydamas šį metodą galėjau laisvai nustatyti žingsnio daugiklį $\gamma$ taip, kaip man atrodė tinkama. Ištyręs daugybę internetinių šaltinių, kuriuose nagrinėjamas gradientinis nusileidimas, nustačiau, kad populiariausi buvo šie šeši: 0.001, 0.003, 0.01, 0.03, 0.1 ir 0.3. Kaip jau minėjau, žiūrėdamas į šias $\gamma$ jau abejojau, kad kai kurios iš jų pasieks minimalų tašką, todėl šiam algoritmui nusprendžiau padidinti didžiausią iteracijų skaičių iki 1000, tačiau, kaip pamatysime, net ir to nepakako. 
\subsubsection{Minimumo taškas ir funkcijos reikšmė}
\begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tblr}{
      cells = {c},
      cell{1}{1} = {c=2,r=3}{0.092\linewidth},
      cell{1}{3} = {c=6}{0.839\linewidth},
      cell{2}{3} = {c=2}{0.219\linewidth},
      cell{2}{5} = {c=2}{0.31\linewidth},
      cell{2}{7} = {c=2}{0.31\linewidth},
      cell{4}{1} = {r=6}{},
      vlines,
      hline{1,4,10} = {-}{},
      hline{2-3} = {3-8}{},
      hline{5-9} = {2-8}{},
    }
                                                        &       & Nuliniai taškai $x_{0}$ &                &                        &                &                        &                \\
                                                        &       & {[}0,0]            &                & {[}1,1]                &                & {[}0.5;0.7]            &                \\
                                                        &       & Min. taškas        & Funkc. reikšmė & Min. taškas            & Funkc. reikšmė & Min. taškas            & Funkc. reikšmė \\
    \begin{sideways}Žingsnio daugikliai $\gamma$\end{sideways} & 0.001 & 0                  & 0              & {[}0.809672, 0.809672] & 0.050753       & {[}0.447687, 0.651397] & 0.003611       \\
                                                        & 0.01  & 0                  & 0              & {[}0.411849, 0.411849] & -0.003738      & {[}0.297361, 0.477088] & -0.003999      \\
                                                        & 0.1   & 0                  & 0              & {[}0.333891, 0.333891] & -0.004629      & {[}0.330972, 0.335728] & -0.004629      \\
                                                        & 0.003 & 0                  & 0              & {[}0.615093, 0.615093] & 0.010886       & {[}0.380095, 0.585160] & -0.000965      \\
                                                        & 0.03  & 0                  & 0              & {[}0.338583, 0.338583] & -0.004626      & {[}0.296528, 0.383428] & -0.004548      \\
                                                        & 0.3   & 0                  & 0              & {[}0.333881, 0.333881] & -0.004629      & {[}0.331648, 0.335034] & -0.004629      
    \end{tblr}
    }
\end{table}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc0_0.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc1_1g0_001.png}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc1_1g0_003.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc1_1g0_01.png}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc1_1g0_03.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc1_1g0_1.png}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc1_1g0_3.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc05_07g0_001.png}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc05_07g0_003.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc05_07g0_01.png}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc05_07g0_03.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc05_07g0_1.png}
        \includegraphics[width=0.72\textwidth]{img/gradDescent/graddesc05_07g0_3.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\subsubsection{Greitis}
\begin{table}[H]
    \centering
    \begin{tblr}{
      cells = {c},
      cell{1}{1} = {c=2,r=3}{},
      cell{1}{3} = {c=3}{},
      cell{4}{1} = {r=6}{},
      vlines,
      hline{1,4,10} = {-}{},
      hline{2-3} = {3-5}{},
      hline{5-9} = {2-5}{},
    }
                                                        &       & Nuliniai taškai $x_{0}$ &               &               \\
                                                        &       & {[}0,0]            & {[}1,1]       & {[}0.5;0.7]   \\
                                                        &       & Iteracijų sk.      & Iteracijų sk. & Iteracijų sk. \\
    \begin{sideways}Žingsnio daugikliai $\gamma$\end{sideways} & 0.001 & 0                  & 1000+         & 1000+         \\
                                                        & 0.01  & 0                  & 1000+         & 1000+         \\
                                                        & 0.1   & 0                  & 475           & 1000+         \\
                                                        & 0.003 & 0                  & 1000+         & 1000+         \\
                                                        & 0.03  & 0                  & 1000+         & 1000+         \\
                                                        & 0.3   & 0                  & 156           & 359           
    \end{tblr}
\end{table}
\subsubsection{Efektyvumas}
\begin{table}[H]
    \centering
    \begin{tblr}{
      cells = {c},
      cell{1}{1} = {c=2,r=3}{},
      cell{1}{3} = {c=3}{},
      cell{4}{1} = {r=6}{},
      vlines,
      hline{1,4,10} = {-}{},
      hline{2-3} = {3-5}{},
      hline{5-9} = {2-5}{},
    }
                                                        &       & Nuliniai taškai $x_{0}$ &                  &                  \\
                                                        &       & {[}0,0]            & {[}1,1]          & {[}0.5;0.7]      \\
                                                        &       & Funkc. iškv. sk.   & Funkc. iškv. sk. & Funkc. iškv. sk. \\
    \begin{sideways}Žingsnio daugikliai $\gamma$\end{sideways} & 0.001 & 2                  & 1001+            & 1001+            \\
                                                        & 0.01  & 2                  & 1001+            & 1001+            \\
                                                        & 0.1   & 2                  & 477              & 1001+            \\
                                                        & 0.003 & 2                  & 1001+            & 1001+            \\
                                                        & 0.03  & 2                  & 1001+            & 1001+            \\
                                                        & 0.3   & 2                  & 158              & 361              
    \end{tblr}
\end{table}
\subsection{Greičiausio nusileidimo metodo rezultatai ir vizualizacija}
\subsubsection{Minimumo taškas ir funkcijos reikšmė}
\begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tblr}{
      cells = {c},
      cell{1}{1} = {c=2,r=3}{},
      cell{1}{3} = {c=6}{},
      cell{2}{3} = {c=2}{},
      cell{2}{5} = {c=2}{},
      cell{2}{7} = {c=2}{},
      cell{4}{1} = {r=3}{},
      vlines,
      hline{1,4,7} = {-}{},
      hline{2-3} = {3-8}{},
      hline{5-6} = {2-8}{},
    }
                                                  &             & Auksinio pjūvio algoritmo intervalai             &                &                        &                &                        &                \\
                                                  &             & {[}0;1]                &                & {[}0;7]                &                & {[}0;20]               &                \\
                                                  &             & Min. taškas            & Funkc. reikšmė & Min. taškas            & Funkc. reikšmė & Min. taškas            & Funkc. reikšmė \\
    \begin{sideways}Nuliniai taškai $x_{0}$\end{sideways} & {[}0,0]     & 0                      & 0              & 0                      & 0              & 0                      & 0              \\
                                                  & {[}1,1]     & {[}0.333860, 0.333860] & -0.004629      & {[}0.333330, 0.333330] & -0.004629      & -                      & -              \\
                                                  & {[}0.5,0.7] & {[}0.331707, 0.334974] & -0.004629      & {[}0.331957, 0.334717] & -0.004629      & {[}0.332230, 0.333935] & -0.004629      
    \end{tblr}
    }
\end{table}

\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/steepDescent/steepdesc0_0.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/steepDescent/steepdesc1_1interv0_1.png}
        \includegraphics[width=0.72\textwidth]{img/steepDescent/steepdesc1_1interv0_7.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/steepDescent/steepdesc05_07interv0_1.png}
        \includegraphics[width=0.72\textwidth]{img/steepDescent/steepdesc05_07interv0_20.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}


\subsubsection{Greitis}
\begin{table}[H]
    \centering
    \begin{tblr}{
      cells = {c},
      cell{1}{1} = {c=2,r=3}{},
      cell{1}{3} = {c=3}{},
      cell{4}{1} = {r=3}{},
      vlines,
      hline{1,4,7} = {-}{},
      hline{2-3} = {3-5}{},
      hline{5-6} = {2-5}{},
    }
                                                     &             & Auksinio pjūvio algoritmo intervalai    &               &               \\
                                                     &             & {[}0;1]       & {[}0;7]       & {[}0;20]      \\
                                                     &             & Iteracijų sk. & Iteracijų sk. & Iteracijų sk. \\
    \begin{sideways}Nuliniai taškai $x_{0}$\end{sideways} & {[}0,0]     & 0             & 0             & 0             \\
                                                     & {[}1,1]     & 44            & 1             & -             \\
                                                     & {[}0.5,0.7] & 107           & 14            & 7             
    \end{tblr}
\end{table}
\subsubsection{Efektyvumas}
\begin{table}[H]
    \centering
    \begin{tblr}{
      cells = {c},
      cell{1}{1} = {c=2,r=3}{},
      cell{1}{3} = {c=3}{},
      cell{4}{1} = {r=3}{},
      vlines,
      hline{1,4,7} = {-}{},
      hline{2-3} = {3-5}{},
      hline{5-6} = {2-5}{},
    }
                                                     &             & Auksinio pjūvio algoritmo intervalai       &                  &                  \\
                                                     &             & {[}0;1]          & {[}0;7]          & {[}0;20]         \\
                                                     &             & Funkc. iškv. sk. & Funkc. iškv. sk. & Funkc. iškv. sk. \\
    \begin{sideways}Nuliniai taškai $x_{0}$\end{sideways} & {[}0,0]     & 2                & 2                & 2                \\
                                                     & {[}1,1]     & 1014             & 29               & -                \\
                                                     & {[}0.5,0.7] & 2463             & 380              & 205              
    \end{tblr}
    \end{table}
\subsection{Deformuojamo simplekso (Nelder-Mead) metodo rezultatai ir vizualizacija}
\subsubsection{Minimumo taškas ir funkcijos reikšmė}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|} 
    \hline
    Nuliniai taškai $x_{0}$ & Min. taškas            & Funkc. reikšmė  \\ 
    \hline
    {[}0,0]            & {[}0.333282, 0.333380] & -0.004629       \\ 
    \hline
    {[}1,1]            & {[}0.333349, 0.333354] & -0.004629       \\ 
    \hline
    {[}0.5,0.7]        & {[}0.333368, 0.333323] & -0.004629       \\
    \hline
    \end{tabular}
\end{table}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\textwidth]{img/simplex/simplex0_0.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}
\begin{figure}[H]
    \begin{adjustbox}{margin=0pt,center}
        \includegraphics[width=0.72\linewidth]{img/simplex/simplex1_1.png}%
        \includegraphics[width=0.72\linewidth]{img/simplex/simplex05_07.png}
    \end{adjustbox}
    %\caption{Niutono metodo vizualizacija (\ref{eq:1}) tikslo funkcijai.}
    %\label{fig:3}
\end{figure}

\subsubsection{Greitis}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|} 
    \hline
    Nuliniai taškai $x_{0}$ & Iteracijų sk.  \\ 
    \hline
    {[}0,0]            & 38             \\ 
    \hline
    {[}1,1]            & 51             \\ 
    \hline
    {[}0.5,0.7]        & 35             \\
    \hline
    \end{tabular}
\end{table}
\subsubsection{Efektyvumas}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|} 
    \hline
    Nuliniai taškai $x_{0}$ & Funkc. iškv. sk.  \\ 
    \hline
    {[}0,0]            & 76                \\ 
    \hline
    {[}1,1]            & 96                \\ 
    \hline
    {[}0.5,0.7]        & 70                \\
    \hline
    \end{tabular}
\end{table}
\subsection{Trijų algoritmų palyginamoji analizė}
\subsubsection{Minimumo taškas ir funkcijos reikšmė}
\subsubsection{Greitis}
\subsubsection{Efektyvumas}
\section{Išvada}
\section{Priedas}

\end{document}